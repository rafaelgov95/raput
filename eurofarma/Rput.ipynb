{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, IncrementalPCA, KernelPCA, SparsePCA, FastICA, FactorAnalysis\n",
    "import umap.umap_ as umap_\n",
    "import hdbscan\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding, Isomap, SpectralEmbedding, MDS\n",
    "import matplotlib.patches as mpatches\n",
    "import re\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from unidecode import unidecode\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import nltk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "import numpy as np\n",
    "import skfuzzy as fuzz"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T18:38:39.317764966Z",
     "start_time": "2023-05-24T18:38:02.777529135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names_out()\n",
    "    labels_ = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels_)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                   .Doc\n",
    "                   .count()\n",
    "                   .reset_index()\n",
    "                   .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                   .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range).fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "\n",
    "def map_labels_to_colors(labels,cmap_):\n",
    "    cmap = plt.get_cmap(cmap_)\n",
    "    num_labels = np.max(labels) + 1\n",
    "    colors = cmap(np.linspace(0, 1, num_labels))\n",
    "    # print(colors)\n",
    "    return colors[labels]\n",
    "\n",
    "\n",
    "def gera_comparacao(list_preprossing_,cluster_):\n",
    "    docs_df = pd.DataFrame(list_preprossing_,columns=[\"Doc\"])\n",
    "    docs_df['Topic'] = cluster_\n",
    "    docs_df['Doc_ID'] = range(len(docs_df))\n",
    "    docs_per_topic=docs_df.groupby([\"Topic\"],as_index=False).agg({\"Doc\":' '.join})\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(list_preprossing_))\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=5)\n",
    "    topic_sizes = extract_topic_sizes(docs_df)\n",
    "    legends=[top_n_words[i][0] for i in top_n_words]\n",
    "    return  legends\n",
    "\n",
    "\n",
    "\n",
    "def silhouette(cluster_labels,x,save):\n",
    "    n_clusters=len(set(cluster_labels))\n",
    "    silhouette_avg = silhouette_score(x, cluster_labels)\n",
    "    sample_silhouette_values = silhouette_samples(x, cluster_labels)\n",
    "    valor_aceitavel = 0.7\n",
    "    if silhouette_avg > valor_aceitavel:\n",
    "        save =save+\"___APROVADO___\"+str(silhouette_avg*100)+\"__\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlim([-0.1, 1])\n",
    "        ax.set_ylim([0, len(x) + (n_clusters + 1) * 10])\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "            colors =mpl.colormaps[\"Spectral\"]\n",
    "            color= colors(float(i) , n_clusters)\n",
    "            ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "            ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "            y_lower = y_upper + 10\n",
    "\n",
    "        ax.set_title(\"Gráfico de Silhuetas para {} clusters\".format(n_clusters))\n",
    "        ax.set_xlabel(\"Valores de Silhueta\")\n",
    "        ax.set_ylabel(\"Cluster\")\n",
    "\n",
    "        ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "        ax.set_yticks([])\n",
    "        fig.savefig(save+\".png\")\n",
    "        plt.close(fig)\n",
    "        return True, silhouette_avg * 100\n",
    "    return False,0\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopwords_ = set(stopwords.words('portuguese'))\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Baixe os recursos necessários do NLTK\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "\n",
    "# Carregue o modelo em português do spaCy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Processa o texto com o modelo do spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lematiza cada token no texto\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "    # Junte os lemas em uma única string\n",
    "    lemmatized_text = ' '.join(lemmas)\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Conversão para minúsculas\n",
    "    text = text.lower()\n",
    "    # Remoção de pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remoção de caracteres especiais, preservando letras acentuadas\n",
    "    text = re.sub(r'[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôûÂÊÎÔÛàèìòùÀÈÌÒÙãõñÃÕÑçÇ\\s]+', '', text)\n",
    "    text = unidecode(text)\n",
    "    return lemmatize_text(text)\n",
    "\n",
    "texto = \"Eu estou correndo no parque. Está um belo dia ensolarado!\"\n",
    "lemmatized_text = preprocess_text(texto)\n",
    "print(lemmatized_text)\n",
    "# !python -m spacy download pt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def plot_info(reduce_embedding,metrics_,kmeans_options,name,modelo_direct,reduce,reduc_op):\n",
    "        for met_ in metrics_:\n",
    "            metrics[met_]=[]\n",
    "        for n_init in kmeans_options['n_init']:\n",
    "            for n_cluster in kmeans_options['n_clusters']:\n",
    "                print(f\"Kmeans debug: {n_init}__cluster_{n_cluster}\")\n",
    "                kmeans = KMeans(n_clusters=n_cluster,n_init=n_init).fit(reduce_embedding)\n",
    "                if len(set(kmeans.labels_)) >1:\n",
    "                    valor_aceite = silhouette(kmeans.labels_,reduce_embedding,f\"./{name}/{modelo_direct}/silhouette/kmeans_{n_cluster}_{reduce}__{reduc_op}\")\n",
    "                    if valor_aceite[0]:\n",
    "                        result = pd.DataFrame(reduce_embedding,columns=['x', 'y'])\n",
    "                        result['labels'] = kmeans.labels_\n",
    "                        silhouette_avg = silhouette_score(reduce_embedding, kmeans.labels_)\n",
    "\n",
    "                        calinski_score = calinski_harabasz_score(reduce_embedding, kmeans.labels_)\n",
    "                        davies_score = davies_bouldin_score(reduce_embedding, kmeans.labels_)\n",
    "                        metrics['silhouette_scores'].append(silhouette_avg)\n",
    "                        metrics['calinski_scores'].append(calinski_score)\n",
    "                        metrics['davies_bouldin_scores'].append(davies_score)\n",
    "                        centroids = kmeans.cluster_centers_\n",
    "                        labels = gera_comparacao(list_preprossing, kmeans.labels_)\n",
    "                        fig, ax = plt.subplots(figsize=figsize)\n",
    "                        plt.scatter(result.x, result.y, c=result.labels, cmap=color_maps)\n",
    "                        distances = pairwise_distances(result[['x', 'y']], centroids)\n",
    "                        avg_distance = np.mean(distances, axis=1)\n",
    "\n",
    "                        radius_factor = 0.2\n",
    "                        adjusted_radius = avg_distance * radius_factor\n",
    "                        for centroid, radius in zip(centroids, adjusted_radius):\n",
    "                            circle = plt.Circle((centroid[0], centroid[1]), radius=radius, color='red', fill=False)\n",
    "                            ax.add_artist(circle)\n",
    "                            plt.scatter(centroid[0], centroid[1], c='red', s=50, marker='x')\n",
    "\n",
    "                        legend_labels = [f\"{label[0]}: {round(label[1]*100,2)}%\" for label in labels]\n",
    "                        legend_colors = map_labels_to_colors(np.arange(len(legend_labels)), color_maps)\n",
    "                        legend_elements = [mpatches.Patch(color=color, label=label) for color, label in zip(legend_colors, labels)]\n",
    "                        plt.legend(handles=legend_elements)\n",
    "                        plt.legend(legend_elements, legend_labels)\n",
    "                        fig.savefig(f\"./{name}/{modelo_direct}/kmeans/kmeans_init_{n_init}_cluster_{n_cluster}_method_reduce_{reduce}_opt{reduc_op}.png\")\n",
    "                        plt.close(fig)\n",
    "                        save_relatorio(name,modelo_direct,f\"kmeans_init{n_init}_\",n_cluster,reduce,reduc_op,valor_aceite[1])\n",
    "\n",
    "\n",
    "def plot_info_hdbscan(reduce_embedding,hdbscan_options,name,modelo_direct,reduce,reduc_op=\"none\"):\n",
    "    for k in hdbscan_options['min_cluster_size']:\n",
    "        hdbscan_ = hdbscan.HDBSCAN(min_cluster_size=k,\n",
    "                                      metric='euclidean',\n",
    "                                      cluster_selection_method='eom').fit(reduce_embedding)\n",
    "        result = pd.DataFrame(reduce_embedding,columns=['x', 'y'])\n",
    "        result['labels'] = hdbscan_.labels_\n",
    "\n",
    "        if len(set(hdbscan_.labels_)) >=1 and result[result['labels']==-1].shape[0] < (result.shape[0]*0.20):\n",
    "            valor_aceite = silhouette(hdbscan_.labels_,reduce_embedding,f\"./{name}/{modelo_direct}/silhouette/hdbscan_{k}_{reduce}__{reduc_op}\")\n",
    "\n",
    "            if valor_aceite[0]:\n",
    "                print(f\"DB Scan{k}\",set(hdbscan_.labels_),\"Count -1: \",result[result['labels']==-1].shape[0],f\" Maximo Permitido: {(result.shape[0]*0.20)}\")\n",
    "                unique_clusters = np.unique(hdbscan_.labels_)\n",
    "                centroids = []\n",
    "                for cluster in unique_clusters:\n",
    "                    if cluster != -1:  # Ignorar pontos de ruído\n",
    "                        cluster_points = result[result.labels == cluster]\n",
    "                        centroid = np.mean(cluster_points[['x', 'y']], axis=0)\n",
    "                        centroids.append(np.array(centroid))\n",
    "                outliers = result.loc[result.labels == -1, :]\n",
    "                clustered = result.loc[result.labels != -1, :]\n",
    "                fig, ax = plt.subplots(figsize=figsize)\n",
    "                plt.scatter(outliers.x, outliers.y, c='black' ,marker='x')\n",
    "                plt.scatter(clustered.x, clustered.y,c=clustered.labels,  cmap=color_maps)\n",
    "                labels = gera_comparacao(list_preprossing,clustered.labels)\n",
    "                distances = pairwise_distances(result[['x', 'y']].to_numpy(), np.array(centroids).reshape(-1, 2))\n",
    "                avg_distance = np.mean(distances, axis=1)\n",
    "                radius_factor = 0.2\n",
    "                adjusted_radius = avg_distance * radius_factor\n",
    "                for centroid, radius in zip(centroids, adjusted_radius):\n",
    "                    circle = plt.Circle((centroid[0], centroid[1]), radius=radius, color='red', fill=False)\n",
    "                    ax.add_artist(circle)\n",
    "                    plt.scatter(centroid[0], centroid[1], c='red', s=50, marker='x')\n",
    "                legend_labels = [f\"{label[0]}: {round(label[1]*100,2)}%\" for label in labels]\n",
    "                legend_colors = map_labels_to_colors(np.arange(len(legend_labels)),color_maps)\n",
    "                legend_elements = [mpatches.Patch(color=color, label=label) for color, label in zip(legend_colors,legend_labels)]\n",
    "                plt.legend(handles=legend_elements)\n",
    "                plt.legend(legend_elements, legend_labels)\n",
    "                fig.savefig(f\"./{name}/{modelo_direct}/hdbscan/hdbscan_{k}_{reduce}___{reduc_op}.png\")\n",
    "                plt.close(fig)\n",
    "                save_relatorio(name,modelo_direct,\"hdbscan\",k,reduce,reduc_op,valor_aceite[1])\n",
    "\n",
    "def save_relatorio (nome_set,modelo_direct,method_group,method_op,reduce,reduce_op,value_s):\n",
    "    new_ = f\"{nome_set}--{modelo_direct}--{method_group}--{method_op}--{reduce}--{reduce_op}:{round(value_s,3)}\\n\"\n",
    "    if os.path.exists(\"./result.txt\"):\n",
    "        with open(\"./result.txt\",\"a\") as documentos:\n",
    "            documentos.write(new_)\n",
    "    else:\n",
    "        with open(\"./result.txt\",\"w+\") as documentos:\n",
    "            documentos.write(new_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def generate_reduce_params(reduces_class):\n",
    "    params_list = []\n",
    "\n",
    "    for reduce in reduces_class:\n",
    "        if 'options' in reduces_class[reduce]:\n",
    "            options = reduces_class[reduce]['options']\n",
    "            param_names = list(options.keys())\n",
    "            param_values = [options[param] for param in param_names]\n",
    "            param_combinations = list(product(*param_values))\n",
    "\n",
    "            for combination in param_combinations:\n",
    "                reduce_params = {'options':{param_names[i]: combination[i] for i in range(len(param_names))}}\n",
    "                reduce_params['method'] = reduces_class[reduce]['method']\n",
    "                params_list.append(reduce_params)\n",
    "\n",
    "    return params_list\n",
    "\n",
    "name_types = {\n",
    "        # \"ativo_classe\":[0,3],\n",
    "    # \"ativo\":[0],\n",
    "    #\n",
    "    #         \"classe\":[3],\n",
    "    #\n",
    "    #             \"ativo_indicacao_contra_indicacao_classes\":[0,1,2,3]\n",
    "    # ,\n",
    " # \"ativo_indicacao\":[0,1],\n",
    "            \"ativo_indicacao_contra_indicacao\":[0,1,2],\n",
    "\n",
    "\n",
    "            \"indicacao\":[1],\n",
    "\n",
    "}\n",
    "\n",
    "# df_train = shuffle(pd.read_csv(\"./medicamentos_reduzidos\"))\n",
    "df_train = pd.read_excel(\"/home/rafael/Documentos/FACOM/Douturado/Doutorado/webcrawler/medicamentos.xlsx\",index_col=0,dtype=str)[:200]\n",
    "df_train.head()\n",
    "color_maps = 'tab20'\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "figsize=(10,5)\n",
    "if os.path.exists(\"./result.txt\"):\n",
    "    os.remove(\"./result.txt\")\n",
    "for name in name_types:\n",
    "    list_preprossing=np.empty(0)\n",
    "    for dta in range(df_train.shape[0]):\n",
    "        value=''\n",
    "        for select in name_types[name]:\n",
    "            value+=str(df_train.iloc[dta,select])\n",
    "        tokens = word_tokenize(preprocess_text(value.replace('nan','')))\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in list(stopwords_)]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        list_preprossing = np.append(list_preprossing,filtered_text)\n",
    "\n",
    "    lnp_models = [\n",
    "\n",
    "        # 'distiluse-base-multilingual-cased-v2',\n",
    "        # 'allenai/scibert_scivocab_uncased',\n",
    "                        # 'all-mpnet-base-v2',\n",
    "                        # 'all-distilroberta-v1',\n",
    "                        # 'neuralmind/bert-base-portuguese-cased',\n",
    "                        # 'distiluse-base-multilingual-cased-v1',\n",
    "\n",
    "                        'all-MiniLM-L12-v2',\n",
    "                        'all-MiniLM-L6-v2',\n",
    "                        'multi-qa-distilbert-dot-v1',\n",
    "                        'multi-qa-distilbert-cos-v1',\n",
    "                        'multi-qa-mpnet-base-dot-v1',\n",
    "                        'multi-qa-MiniLM-L6-cos-v1',\n",
    "                        'multi-qa-MiniLM-L6-dot-v1',\n",
    "                        'paraphrase-multilingual-mpnet-base-v2',\n",
    "                        'paraphrase-albert-small-v2',\n",
    "                        'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                        'paraphrase-MiniLM-L3-v2',\n",
    "    ]\n",
    "    hdbscan_options ={\"min_cluster_size\":[x for x in range(6,20,2)],'metric':['euclidean','cosine']}\n",
    "    kmeans_options ={'n_clusters':[x for x in range(5,30,3)], 'n_init':['auto']}\n",
    "    reduces_class = {\n",
    "    # 'isomap':{\n",
    "    #                             'options':{'n_neighbors':[x for x in range(4,20)],'n_components':2},\n",
    "    #                             'method':Isomap\n",
    "    #                         },\n",
    "                            'umap':{\n",
    "                                'options':{'n_neighbors':[x for x in range(2,10,2)], 'n_components':[2],\n",
    "                                           'metric':['cosine','euclidean']},\n",
    "                                'method':umap_.UMAP\n",
    "                            },\n",
    "                            'tnse':{\n",
    "                                'options':{'perplexity':[x for x in range(1,15,2)], 'n_components':[2],\n",
    "                                           'metric':['cosine','euclidean']},\n",
    "                                'method':TSNE\n",
    "                            },\n",
    "                            # 'efcm':{\n",
    "                            #     'num_clusters' : [x for x in range(2,30,1)],\n",
    "                            #     'fuzziness' : [x for x in range(2,30,1)]\n",
    "                            # },\n",
    "\n",
    "                            # 'lle-standard':{\n",
    "                            #     'options':{'n_neighbors':[x for x in range(2,8)], 'n_components':2 ,\"method\":\"standard\"},\n",
    "                            #     'method':LocallyLinearEmbedding\n",
    "                            # },\n",
    "                            # 'lle-hessian':{\n",
    "                            #     'options':{'n_neighbors':[x for x in range(2,8)], 'n_components':2,\n",
    "                            #     \"method\":\"hessian\"},\n",
    "                            #     'method':LocallyLinearEmbedding\n",
    "                            # },\n",
    "                            # 'lle-modified':{\n",
    "                            #     'options':{'n_neighbors':[x for x in range(5,8)], 'n_components':2,\"method\":\"modified\"},\n",
    "                            #     'method':LocallyLinearEmbedding\n",
    "                            # },\n",
    "                            #\n",
    "                            # 'lle-ltsa':{\n",
    "                            #     'options':{'n_neighbors':[x for x in range(5,15)], 'n_components':2,\n",
    "                            #                \"method\":\"ltsa\"},\n",
    "                            #     'method':LocallyLinearEmbedding\n",
    "                            # },\n",
    "                            # 'random-projection':{\n",
    "                            #     'options':{'n_components':2},\n",
    "                            #     'method':SparseRandomProjection\n",
    "                            # },\n",
    "                            # 'lda ':{\n",
    "                            #     'options':{'n_components':2},\n",
    "                            #     'method':LinearDiscriminantAnalysis\n",
    "                            # },\n",
    "                            #\n",
    "                            # 'mds':{\n",
    "                            #     'options':{'n_components':2},\n",
    "                            #     'method':MDS\n",
    "                            # },\n",
    "                            # 'spectral':{\n",
    "                            #     'options':{ 'n_components':[2],'affinity':['rbf', 'precomputed','precomputed_nearest_neighbors'],'n_neighbors':[2,3,4,5,6,7,8,10]},\n",
    "                            #     'method':SpectralEmbedding\n",
    "                            # },\n",
    "                            # 'kernel_pca':{\n",
    "                            #\n",
    "                            #     'options':{ 'n_components':[2],  'kernel':['linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed']},\n",
    "                            #     'method':KernelPCA\n",
    "                            #\n",
    "                            # },\n",
    "                            # 'truncate_svd':{\n",
    "                            #     'options':{'n_components':[2],'algorithm' :['arpack']},\n",
    "                            #     'method':TruncatedSVD\n",
    "                            # },\n",
    "                            # 'pca':{\n",
    "                            #     'options':{'n_components':2},\n",
    "                            #     'method':PCA\n",
    "                            # },\n",
    "                            # 'factor_analysis':{\n",
    "                            #     'options':{ 'n_components':2},\n",
    "                            #     'method':FactorAnalysis\n",
    "                            # },\n",
    "                            # 'incremental_pca':{\n",
    "                            #     'options':{ 'n_components':2},\n",
    "                            #     'method':IncrementalPCA\n",
    "                            # }\n",
    "                           #,\n",
    "                            # 'sparse_pca':{\n",
    "                            #     'options':{'n_components':[2]},\n",
    "                            #     'method':SparsePCA\n",
    "                            # },\n",
    "                           #  'fast_ica':{\n",
    "                           #      'options':{ 'n_components':2},\n",
    "                           #      'method':FastICA\n",
    "                           # }\n",
    "                    }\n",
    "    metrics = {\n",
    "                        'silhouette_scores': [],\n",
    "                        'calinski_scores':[],\n",
    "                        'davies_bouldin_scores':[]\n",
    "                }\n",
    "    map_type =[\"silhouette\",\"kmeans\",\"hdbscan\"]\n",
    "\n",
    "    for model_index in lnp_models:\n",
    "        if len(model_index.split('/'))>1:\n",
    "            modelo_direct = model_index.split('/')[1]\n",
    "        else:\n",
    "            modelo_direct = model_index\n",
    "        if os.path.exists(f'./{name}'):\n",
    "            if os.path.exists(f\"./{name}/{modelo_direct}\"):\n",
    "                shutil.rmtree(f\"{name}/{modelo_direct}\")\n",
    "                os.makedirs(f\"{name}/{modelo_direct}\")\n",
    "            else:\n",
    "                os.makedirs(f\"./{name}/{modelo_direct}\")\n",
    "        else:\n",
    "            os.makedirs(f\"./{name}\")\n",
    "            os.makedirs(f\"./{name}/{modelo_direct}\")\n",
    "\n",
    "        # for m in metrics:\n",
    "        #     if os.path.exists(f'./{name}/{modelo_direct}/{m}'):\n",
    "        #         shutil.rmtree(f\"{name}/{modelo_direct}/{m}\")\n",
    "        #         os.makedirs(f\"{name}/{modelo_direct}/{m}\")\n",
    "        #     else:\n",
    "        #         os.m        # break\n",
    "    # breakakedirs(f\"{name}/{modelo_direct}/{m}\")\n",
    "\n",
    "        for map in map_type:\n",
    "            if os.path.exists(f'./{name}/{modelo_direct}/{map}'):\n",
    "                shutil.rmtree(f\"{name}/{modelo_direct}/{map}\")\n",
    "                os.makedirs(f\"{name}/{modelo_direct}/{map}\")\n",
    "            else:\n",
    "                os.makedirs(f\"{name}/{modelo_direct}/{map}\")\n",
    "\n",
    "        corpus_embeddings = SentenceTransformer(model_index).encode(list_preprossing,show_progress_bar=True)\n",
    "        reduces_compile=generate_reduce_params(reduces_class)\n",
    "        print(reduces_compile)\n",
    "        for reduce in reduces_compile:\n",
    "            print(f\"Modelo:{model_index} reduce:{reduce}\")\n",
    "            reduce_embedding=reduce['method'](**reduce['options']).fit_transform(corpus_embeddings)\n",
    "            plot_info(reduce_embedding,metrics,kmeans_options,name,modelo_direct,reduce,'')\n",
    "            plot_info_hdbscan(reduce_embedding,hdbscan_options,name,modelo_direct,reduce,'')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
